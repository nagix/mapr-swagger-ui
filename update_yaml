#!/usr/bin/python

from lxml import etree
from pyquery import PyQuery as pq
import os
import re
import sys

filename = 'mapr.yaml'
baseurl = 'http://maprdocs.mapr.com/home/ReferenceGuide/maprcli-REST-API-Syntax.html'

# Escape HTML text
# MapR doc bug: u'\u2013' should be '-'
def escape(text):
  return (text or '').replace('<', '&lt;').replace('>', '&gt;').replace(u'\u2013', '-')

# Remove unnecessary spaces
def trim(text):
  return re.sub(r'\s+', ' ', text)

# Adjust spaces and line breaks
def normalize(d):
  pre_descendant = d('pre *')
  for e in d('*'):
    if e in pre_descendant:
      e.text = escape(e.text).replace('\n', '\n\n')
      e.tail = escape(e.tail).replace('\n', '\n\n')
    else:
      if e.tag == 'pre':
        e.text = escape(e.text).replace('\n', '\n\n')
      else:
        e.text = trim(escape(e.text))
      e.tail = trim(escape(e.tail))
  return d

# Replace HTML tags with Markdown
def replace_tags(d, selector, start, end, a_selector=None, a_start=None, a_end=None):
  elements = d.find(selector)
  for e in reversed(elements):
    start_tag, end_tag = start, end
    if a_selector:
      if replace_tags(pq(e), a_selector, a_start, a_end) and a_selector == 'a':
        start_tag, end_tag = '', ''
    html = escape(e.text)
    children = e.getchildren()
    if children:
      html += unicode('').join([etree.tostring(c) for c in children])
    href = e.get('href') or ''
    if href:
      href = '(%s)' % href
    tail = escape(e.tail)
    pq(e).before(start_tag + html + end_tag + href + tail)
    parent = e.getparent()
    parent.remove(e)
  return elements

# Get the GFM representation of sub nodes
def gfm(d):
  s = d.clone().wrap_all('<section></section>')
  normalize(s)
  replace_tags(s, 'h2', '\n\n## ', '\n\n')
  replace_tags(s, 'strong','**', '**')
  replace_tags(s, 'code', '`', '`', 'a', '[`', '`]')
  replace_tags(s, 'a', '[', ']')
  replace_tags(s, 'ul ul', '', '', 'li', '\n  * ', '\n')
  replace_tags(s, 'ul', '\n', '\n', 'li', '\n* ', '\n')
  replace_tags(s, 'ol', '\n', '\n', 'li', '\n1. ', '\n')
  replace_tags(s, 'section,td,p,div', '\n\n\n', '\n\n\n')
  replace_tags(s, 'pre', '\n\n\n```\n\n', '\n\n```\n\n\n')
  replace_tags(s, 'span', '', '')
  s('*').remove_attr('class')
  return (s.html() or '').strip()

# Parse enum
def parse_enum(name, d, syntax):

  # Parse from syntax
  match = re.search(r'-%s[^\]\n]+?([\w \|]+\| *\w+)' % name, syntax)
  if match:
    enum = []
    for i in match.group(1).split('|'):
      if re.search(r'\W', i.strip()):
        break
      enum.append(i.strip())
    else:
      return enum

  # Parse from a list
  if d('ul'):
    enum = []
    for i in d('ul:first li').items():
      value = re.search(r'\S(?:[\w\s/]*[\w*])?', i.text(), re.S).group()
      if re.search(r'\W', value):
        break
      enum.append(value)
    else:
      return enum

  # Parse from a text
  match = None
  if name in ['output', 'replicationtype', 'type']:
    match = re.search(r'(\w+)(?: \(.+?\))? or (\w+)', d.text(), re.S)
  elif name in ['tabletype']:
    match = re.search(r'(\w+) and (\w+)', d.text(), re.S)  
  if match:
    return match.groups()

  return []

# Detect type
def detect_type(name, description, syntax):
  if re.search(r'-%s[^\]\n]+(true|false)' % name, syntax):
    return 'boolean'
  elif re.search(r'default\s+is\s+(true|false)', description):
    return 'boolean'
  return 'string'

# Print headers
def print_headers(d):

  # Print info
  print('swagger: "2.0"')
  print('info:')
  print('  title: MapR API')
  print('  description: The information about the MapR REST API')
  print('  contact:')
  print('    url: http://maprdocs.mapr.com')

  # Print version
  version = d('meta[name=version]').attr.content
  print('  version: "%s"' % version)

  # Print other headers
  print('basePath: /rest')
  print('schemes:')
  print('  - https')
  print('produces:')
  print('  - application/json')
  print('securityDefinitions:')
  print('  BasicAuth:')
  print('    type: basic')
  print('security:')
  print('  - BasicAuth: []')
  print('paths:')

# Print command
def print_command(d, tag):

  # Get command path
  syntax = d('h2,p')('*:contains("Syntax") ~ *').text()
  if not syntax:
    syntax = d('dt:contains("REST")').parent().text()
  match = re.search('(?<=rest)/[a-z\-/<>]+', syntax)
  if not match:
    return
  path = match.group().replace('<','{').replace('>','}')

  # Handle MapR doc bug
  if 'acl-set' in d.base_url:
    path = path.replace('edit', 'set')
  if 'stream_edit' in d.base_url:
    path = path.replace('create', 'edit')
  if 'stream_upstream_remove' in d.base_url:
    path = path.replace('add', 'remove')
  if 'volume-link-create' in d.base_url:
    path = path.replace('remove', 'create')

  # Print progress
  sys.stderr.write('\r\033[KProcessing %s...' % path)

  # Print summary and description
  short_desc = d('.shortdesc') or d('p:first')
  summary = trim(short_desc.text())
  if short_desc.next().find('p:contains("Syntax")'):
    content = short_desc.next().children()
  else:
    content = short_desc.next_all()
  elements = []
  for e in content:
    if pq(e)('h2,p')('*:contains("Syntax")'):
      break
    if pq(e)('dt:contains("REST")'):
      break
    elements.append(e)
  description = gfm(pq(elements))
  print('  %s:' % path)
  print('    get:')
  print('      tags:')
  print('        - %s' % tag)
  print('      summary: "%s"' % summary)
  print('      description: "%s"' % description)
  print('      externalDocs:')
  print('        url: %s' % d.base_url)

  # Print parameters
  parameters = d('h2,p')('*:contains("Parameters") ~ * tbody').eq(0)
  if not parameters:
    parameters = d('th:contains("Parameter")').closest('table').find('tbody')
  if parameters:
    print('      parameters:')
    for i in parameters('tr').items():
      parameter = [j for j in i('td').items()]
      name = trim(parameter[0].text())

      # Handle MapR doc bug
      if path == '/rlimit/set' and name == 'limit':
        name = 'value'
      elif name == 'virtualip end range':
        name = 'virtualipend'
      elif name == 'mac':
        name = 'tomac'
      elif name == 'clustername':
        name = 'cluster'
      else:
        name = re.search(r'\w+', name).group()
      if name == 'json' or name == 'long':
        continue
      name = name[0].lower() + name[1:]

      in_ = 'query'
      description = gfm(parameter[1])
      required = parameter[0]('strong')
      type = detect_type(name, parameter[1].text(), syntax)
      enum = parse_enum(name, parameter[1], syntax)
      if 'urls' in d.base_url and name == 'name':
        in_ = 'path'
        required = True
      print('        - name: %s' % name)
      print('          in: %s' % in_)
      if '"' in description:
        print('          description: \'%s\'' % description)
      else:
        print('          description: "%s"' % description)
      if required:
        print('          required: true')
      print('          type: %s' % type)
      if enum:
        print('          enum:')
        for value in enum:
          print('            - %s' % value)

  # Print responses
  print('      responses:')
  print('        200:')
  print('          description: OK')

# Find and print command
def find_command(links, cat=None):
  for e in links:
    category = cat
    if not cat:
      category = e.text
    d = pq(e.get('href')).make_links_absolute()
    sublinks = d('.link a')
    if sublinks:
      find_command(sublinks, category)
    else:
      print_command(d, category)

# Main
with open(os.path.join(os.path.dirname(__file__), filename), "w") as sys.stdout:
  d = pq(baseurl).make_links_absolute()
  print_headers(d)
  find_command(d('.link:gt(0) a'))

sys.stderr.write('\r\033[K')
sys.stderr.flush()
