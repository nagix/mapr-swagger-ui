#!/usr/bin/python

from lxml import etree
from pyquery import PyQuery as pq
from collections import OrderedDict
import json
import os
import re
import sys

filename = 'mapr.json'
baseurl = 'http://maprdocs.mapr.com/home/ReferenceGuide/maprcli-REST-API-Syntax.html'
enums = {}

# Escape HTML text
def escape(text):
  return (text or '').replace('<', '&lt;').replace('>', '&gt;')

# Remove unnecessary spaces
def trim(text):
  return re.sub(r'\s+', ' ', text)

# Adjust spaces and line breaks
def normalize(d):
  pre_descendant = d('pre *')
  for e in d('*'):
    if e not in pre_descendant:
      if e.tag != 'pre':
        e.text = trim(escape(e.text))
      e.tail = trim(escape(e.tail))
  return d

# Replace HTML tags with Markdown
def replace_tags(d, selector, start, end):
  elements = d.find(selector)
  for e in reversed(elements):
    html = escape(e.text)
    children = e.getchildren()
    if children:
      html += unicode('').join([etree.tostring(c) for c in children])
    href = e.get('href') or ''
    if href:
      href = '(%s)' % href
    tail = escape(e.tail)
    pq(e).before(start + html + end + href + tail)
    parent = e.getparent()
    parent.remove(e)
  return elements

# Get the GFM representation of sub nodes
def gfm(d):
  s = d.clone().wrap_all('<section></section>')
  normalize(s)
  replace_tags(s, 'h2', '\n\n## ', '\n\n')
  replace_tags(s, 'strong','**', '**')
  replace_tags(s, 'code a', '[`', '`]')
  replace_tags(s, 'pre code,code code,code:contains("`")', '', '')
  replace_tags(s, 'code', '`', '`')
  replace_tags(s, 'a', '[', ']')
  replace_tags(s, 'ul dl', '', '')
  replace_tags(s, 'ul dt', '**', '**<br>')
  replace_tags(s, 'ul dd', '', '<br>')
  replace_tags(s, 'dl', '', '\n\n')
  replace_tags(s, 'dt', '\n\n**', '**')
  replace_tags(s, 'dd', '\n\n', '')
  replace_tags(s, 'ul ul li', '\n  * ', '')
  replace_tags(s, 'ul ul', '', '')
  replace_tags(s, 'ul li', '\n* ', '')
  replace_tags(s, 'ol li', '\n1. ', '')
  replace_tags(s, 'ul,ol', '\n', '\n\n')
  replace_tags(s, 'section,td,p,div', '\n\n', '\n\n')
  replace_tags(s, 'pre', '\n\n```\n', '\n```\n\n')
  replace_tags(s, 'span', '', '')
  s('*').remove_attr('class')
  return (s.html() or '').strip()

# Parse enum
def parse_enum(name, d, syntax):

  # Parse from syntax
  match = re.search(r'-%s[^\]\n]+?([\w \|]+\| *\w+)' % name, syntax)
  if match:
    enum = []
    for i in match.group(1).split('|'):
      if re.search(r'\W', i.strip()):
        break
      enum.append(i.strip())
    else:
      return enum

  # Parse from a list
  if d('ul'):
    enum = []
    for i in d('ul:first li').items():
      value = re.search(r'\S(?:[\w\s/]*[\w*])?', i.text(), re.S).group()
      if re.search(r'\W', value):
        break
      enum.append(value)
    else:
      return enum

  # Parse from a text
  match = None
  if name in ['output', 'replicationtype', 'type']:
    match = re.search(r'(\w+)(?: \(.+?\))? or (\w+)', d.text(), re.S)
  elif name in ['tabletype']:
    match = re.search(r'(\w+) and (\w+)', d.text(), re.S)  
  if match:
    return match.groups()

  return []

# Detect type
def detect_type(name, description, syntax):
  if re.search(r'-%s[^\]\n]+(true|false)' % name, syntax):
    return 'boolean'
  elif re.search(r'default\s+is\s+(true|false)', description):
    return 'boolean'
  return 'string'

# Parse parameters
def parse_parameters(d, path, syntax):
  parameters_json = []
  parameters = d('h2,p')('*:contains("Parameters") ~ * tbody').eq(0)
  if not parameters:
    parameters = d('th:contains("Parameter")').closest('table').find('tbody')
  if parameters:
    for i in parameters('tr').items():
      parameter = [j for j in i('td').items()]
      name = trim(parameter[0].text())

      if re.search(r'(json|long)$', name):
        continue

      in_ = 'query'
      description = gfm(parameter[1])
      required = len(parameter[0]('strong')) > 0
      if 'urls' in d.base_url and name == 'name':
        in_ = 'path'
        required = True
      type = detect_type(name, parameter[1].text(), syntax)
      enum = parse_enum(name, parameter[1], syntax)
      if path == '/node/list' and name == 'columns':
        type = 'array'
        enum = enums['node']
      elif re.search(r'/volume/(info|list)', path) and name == 'columns':
        type = 'array'
        enum = enums['volume']
      elif path == '/volume/snapshot/list' and name == 'columns':
        type = 'array'
        enum = enums['volume snapshot list']

      parameters_json.append(OrderedDict([
        ('name', name),
        ('in', in_),
        ('description', description),
      ] + ([
        ('required', True),
      ] if required else []) + [
        ('type', type)
      ] + ([
        ('items', {
          'type': 'string'
        }),
        ('collectionFormat', 'csv')
      ] if type == 'array' else []) + ([
        ('enum', enum)
      ] if enum else [])))

  return parameters_json

# Parse command
def parse_command(d, tag):

  # Get command path
  syntax = d('h2,p')('*:contains("Syntax") ~ *').text()
  if not syntax:
    syntax = d('dt:contains("REST")').parent().text()
  match = re.search('(?<=rest)/[a-z\-/<>]+', syntax)
  if not match:
    return {}
  path = match.group().replace('<','{').replace('>','}')

  # Print progress
  sys.stderr.write('\r\033[KProcessing %s...' % path)

  # Parse summary and description
  short_desc = d('.shortdesc') or d('p:first')
  summary = trim(short_desc.text())
  if short_desc.next().find('p:contains("Syntax")'):
    content = short_desc.next().children()
  else:
    content = short_desc.next_all()
  elements = []
  for e in content:
    if pq(e)('h2,p')('*:contains("Syntax")'):
      break
    if pq(e)('dt:contains("REST")'):
      break
    elements.append(e)
  description = gfm(pq(elements))

  return {
    path: {
      'get': OrderedDict([
        ('tags', [tag]),
        ('summary', summary),
        ('description', description),
        ('externalDocs', {
          'url': d.base_url
        }),
        ('parameters', parse_parameters(d, path, syntax)),
        ('responses', {
          '200': {
            'description': 'OK'
          }
        })
      ])
    }
  }

# Get enum from field table
def get_enum(d, tag):
  if tag not in ['node', 'volume', 'volume snapshot list']:
    return
  fields = d('h2')('*:contains("Fields") ~ * tbody').eq(0)
  if fields:
    enums[tag] = []
    for i in fields('tr').items():
      enums[tag].append(i('td:first').text())

# Find and parse command
def find_command(links, cat=None):
  document = OrderedDict()
  for e in links:
    category = cat
    if not cat:
      category = e.text
    d = pq(e.get('href')).make_links_absolute()
    get_enum(d, e.text)
    sublinks = d('.link a')
    if sublinks:
      document.update(find_command(sublinks, category))
    else:
      document.update(parse_command(d, category))
  return document

# Main
d = pq(baseurl).make_links_absolute()
document = OrderedDict([
  ('swagger', '2.0'),
  ('info', OrderedDict([
    ('title', 'MapR API'),
    ('description', 'The information about the MapR REST API'),
    ('contact', {
      'url': 'http://maprdocs.mapr.com'
    }),
    ('version', d('meta[name=version]').attr.content)
  ])),
  ('basePath', '/rest'),
  ('schemes', ['https']),
  ('produces', ['application/json']),
  ('paths', find_command(d('.link:gt(0) a'))),
  ('securityDefinitions', {
    'basic_auth': {
      'type': 'basic'
    }
  }),
  ('security', [{
    'basic_auth': []
  }])
])

with open(os.path.join(os.path.dirname(__file__), filename), "w") as f:
  json.dump(document, f, indent=2)

sys.stderr.write('\r\033[K')
sys.stderr.flush()
